name: Train Model

on:
  pull_request:
    branches:
      - main
    paths:
      - 'WMS/data/training/*.dvc'  # DVC metadata files
      - 'WMS/src/**'
      - 'WMS/configs/**'
  workflow_dispatch:  # Allow manual trigger
    inputs:
      attempts:
        description: 'Number of training attempts'
        required: false
        default: '3'

jobs:
  train:
    runs-on: self-hosted  # Runs on EC2
    strategy:
      max-parallel: 1  # Run attempts sequentially
      fail-fast: false  # Don't stop on first failure
      matrix:
        attempt: [1, 2, 3]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc[s3]

      - name: Configure AWS credentials
        run: |
          # AWS credentials should be configured on EC2 instance via IAM role
          # No need to pass secrets - instance profile handles it
          aws sts get-caller-identity

      - name: Pull training data with DVC
        run: |
          cd WMS/data/training
          dvc pull images.dvc masks.dvc || echo "DVC pull failed, using local data"

      - name: Data Quality Assurance
        run: |
          python devops/scripts/data-qa.py WMS/data/training/ --output data_qa_report.json

          # Display summary
          cat data_qa_report.json

          # Check if passed
          if grep -q '"status": "FAIL"' data_qa_report.json; then
            echo "‚ùå Data QA failed! Check report above."
            exit 1
          fi

          echo "‚úÖ Data QA passed!"

      - name: Train model (Attempt ${{ matrix.attempt }})
        env:
          MLFLOW_TRACKING_URI: http://localhost:5000
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "=== Training Attempt ${{ matrix.attempt }}/3 ==="
          cd WMS/src
          # Use combination of run_number and attempt for unique seed
          SEED=$(((${{ github.run_number }} * 100) + ${{ matrix.attempt }}))
          echo "Using seed: $SEED"
          python train.py --config ../configs/train.yaml --seed $SEED

      - name: Get training results
        id: results
        run: |
          # Extract metrics from latest MLflow run
          python3 << 'EOF'
          import mlflow
          import json
          import os

          mlflow.set_tracking_uri("http://localhost:5000")
          client = mlflow.tracking.MlflowClient()

          # Get latest run from experiment
          experiment = client.get_experiment_by_name("water-meter-segmentation")
          if experiment:
              runs = client.search_runs(
                  experiment_ids=[experiment.experiment_id],
                  order_by=["start_time DESC"],
                  max_results=1
              )

              if runs:
                  run = runs[0]
                  metrics = run.data.metrics

                  # Baseline from original model
                  baseline_dice = 0.9275
                  baseline_iou = 0.8865

                  # Quality gate thresholds (2% tolerance)
                  threshold_dice = 0.9075
                  threshold_iou = 0.8665

                  current_dice = metrics.get('val_dice', 0)
                  current_iou = metrics.get('val_iou', 0)

                  passed = current_dice >= threshold_dice and current_iou >= threshold_iou
                  improved = current_dice > baseline_dice or current_iou > baseline_iou

                  result = {
                      "attempt": ${{ matrix.attempt }},
                      "run_id": run.info.run_id,
                      "metrics": {
                          "dice": current_dice,
                          "iou": current_iou
                      },
                      "baseline": {
                          "dice": baseline_dice,
                          "iou": baseline_iou
                      },
                      "thresholds": {
                          "dice": threshold_dice,
                          "iou": threshold_iou
                      },
                      "passed": passed,
                      "improved": improved
                  }

                  print(json.dumps(result, indent=2))

                  # Save for this attempt
                  filename = f'training_results_attempt_{${{ matrix.attempt }}}.json'
                  with open(filename, 'w') as f:
                      json.dump(result, f, indent=2)

                  # Output for GitHub Actions
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write(f"passed={str(passed).lower()}\n")
                      f.write(f"improved={str(improved).lower()}\n")
                      f.write(f"dice={current_dice}\n")
                      f.write(f"iou={current_iou}\n")
                      f.write(f"run_id={run.info.run_id}\n")
              else:
                  print("No runs found")
                  # Create failure result
                  result = {"attempt": ${{ matrix.attempt }}, "error": "No runs found"}
                  with open(f'training_results_attempt_{${{ matrix.attempt }}}.json', 'w') as f:
                      json.dump(result, f, indent=2)
          else:
              print("Experiment not found")
              # Create failure result
              result = {"attempt": ${{ matrix.attempt }}, "error": "Experiment not found"}
              with open(f'training_results_attempt_{${{ matrix.attempt }}}.json', 'w') as f:
                  json.dump(result, f, indent=2)
          EOF

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: training-results-attempt-${{ matrix.attempt }}
          path: training_results_attempt_${{ matrix.attempt }}.json
          retention-days: 7

  # Aggregate results from all attempts
  aggregate-results:
    needs: train
    if: always()
    runs-on: self-hosted
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: training-results

      - name: Aggregate and evaluate
        id: aggregate
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path

          results_dir = Path('training-results')
          all_results = []

          # Load all attempt results
          for attempt_dir in sorted(results_dir.glob('training-results-attempt-*')):
              json_file = attempt_dir / f'training_results_attempt_{attempt_dir.name.split("-")[-1]}.json'
              if json_file.exists():
                  with open(json_file) as f:
                      all_results.append(json.load(f))

          if not all_results:
              print("No results found")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("any_passed=false\n")
                  f.write("any_improved=false\n")
                  f.write("best_dice=0\n")
                  f.write("best_iou=0\n")
              exit(0)

          # Find best result
          valid_results = [r for r in all_results if 'metrics' in r]
          any_passed = any(r.get('passed', False) for r in valid_results)
          any_improved = any(r.get('improved', False) for r in valid_results)

          best_result = None
          if valid_results:
              # Sort by dice score
              best_result = max(valid_results, key=lambda x: x['metrics']['dice'])

          # Save aggregated results
          aggregated = {
              "total_attempts": len(all_results),
              "successful_attempts": len(valid_results),
              "any_passed": any_passed,
              "any_improved": any_improved,
              "best_result": best_result,
              "all_attempts": all_results
          }

          with open('aggregated_results.json', 'w') as f:
              json.dump(aggregated, f, indent=2)

          print(json.dumps(aggregated, indent=2))

          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"any_passed={str(any_passed).lower()}\n")
              f.write(f"any_improved={str(any_improved).lower()}\n")
              if best_result:
                  f.write(f"best_dice={best_result['metrics']['dice']}\n")
                  f.write(f"best_iou={best_result['metrics']['iou']}\n")
                  f.write(f"best_run_id={best_result['run_id']}\n")
                  f.write(f"best_attempt={best_result['attempt']}\n")
              else:
                  f.write("best_dice=0\n")
                  f.write("best_iou=0\n")
          EOF

      - name: Promote best model to Production
        if: steps.aggregate.outputs.any_improved == 'true'
        run: |
          python3 << 'EOF'
          import mlflow
          from mlflow.tracking import MlflowClient
          import json

          mlflow.set_tracking_uri("http://localhost:5000")
          client = MlflowClient()

          # Load aggregated results
          with open('aggregated_results.json', 'r') as f:
              aggregated = json.load(f)

          best_result = aggregated['best_result']
          run_id = best_result['run_id']

          # Create new model version
          model_version = client.create_model_version(
              name="water-meter-segmentation",
              source=f"runs:/{run_id}/model",
              run_id=run_id
          )

          print(f"Created model version: {model_version.version}")

          # Transition to Production
          client.transition_model_version_stage(
              name="water-meter-segmentation",
              version=model_version.version,
              stage="Production",
              archive_existing_versions=True
          )

          print(f"Model version {model_version.version} from attempt {best_result['attempt']} promoted to Production!")
          EOF

      - name: Comment on PR with all results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let aggregated;
            try {
              aggregated = JSON.parse(fs.readFileSync('aggregated_results.json', 'utf8'));
            } catch (e) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body: '‚ùå **Training failed** - could not read aggregated results'
              });
              return;
            }

            const anyPassed = aggregated.any_passed;
            const anyImproved = aggregated.any_improved;
            const bestResult = aggregated.best_result;

            // Build attempts table
            let attemptsTable = '| Attempt | Dice | IoU | Passed | Improved |\n|---------|------|-----|--------|----------|\n';
            for (const result of aggregated.all_attempts) {
              if (result.metrics) {
                const dice = result.metrics.dice.toFixed(4);
                const iou = result.metrics.iou.toFixed(4);
                const passed = result.passed ? '‚úÖ' : '‚ùå';
                const improved = result.improved ? 'üìà' : '-';
                const highlight = bestResult && result.attempt === bestResult.attempt ? ' üèÜ' : '';
                attemptsTable += `| ${result.attempt}${highlight} | ${dice} | ${iou} | ${passed} | ${improved} |\n`;
              } else {
                attemptsTable += `| ${result.attempt} | - | - | ‚ùå | - |\n`;
              }
            }

            const overallStatus = anyPassed ? '‚úÖ' : '‚ùå';
            const improvementBadge = anyImproved ? 'üìà **MODEL IMPROVED**' : 'üìä No improvement';

            const body = `
            ## ${overallStatus} Training Results (${aggregated.total_attempts} attempts)

            ${improvementBadge}

            ### All Attempts

            ${attemptsTable}

            üèÜ = Best result

            ### Best Result (Attempt ${bestResult ? bestResult.attempt : 'N/A'})

            | Metric | Value | Baseline | Threshold | Status |
            |--------|-------|----------|-----------|--------|
            | **Dice** | ${bestResult ? bestResult.metrics.dice.toFixed(4) : 'N/A'} | 0.9275 | 0.9075 | ${bestResult && bestResult.metrics.dice >= 0.9075 ? '‚úÖ' : '‚ùå'} |
            | **IoU** | ${bestResult ? bestResult.metrics.iou.toFixed(4) : 'N/A'} | 0.8865 | 0.8665 | ${bestResult && bestResult.metrics.iou >= 0.8665 ? '‚úÖ' : '‚ùå'} |

            ### Quality Gate
            - **Any attempt passed**: ${anyPassed ? 'YES ‚úÖ' : 'NO ‚ùå'}
            - **Any attempt improved**: ${anyImproved ? 'YES üìà' : 'NO'}

            ${anyImproved ? 'üöÄ **Best model has been promoted to Production**' : '‚ö†Ô∏è No improvement - model will not be deployed'}

            ---
            ü§ñ Generated by [Claude Code](https://claude.com/claude-code)
            `;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body: body
            });

      - name: Auto-approve PR if improved
        if: steps.aggregate.outputs.any_improved == 'true' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            try {
              await github.rest.pulls.createReview({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: context.payload.pull_request.number,
                event: 'APPROVE',
                body: '‚úÖ Auto-approved: Model improved over baseline\n\nü§ñ This PR has been automatically approved because training produced a model that improves upon the baseline metrics.'
              });
              console.log('PR auto-approved!');
            } catch (error) {
              console.log('Could not auto-approve PR:', error.message);
              console.log('Note: Bot account may not have permission to approve PRs');
            }

      - name: Fail workflow if no improvement
        if: steps.aggregate.outputs.any_improved != 'true'
        run: |
          echo "‚ùå No training attempt improved the model!"
          echo "All 3 attempts completed but none exceeded the baseline metrics."
          echo ""
          echo "Options:"
          echo "1. Review training logs for issues"
          echo "2. Check if new training data is sufficient"
          echo "3. Consider adjusting hyperparameters"
          echo "4. Close this PR and try again with more/better data"
          exit 1
